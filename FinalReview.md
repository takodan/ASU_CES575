# U5 Quiz
1. 圖模型
    1. 用來可視化隨機變量關係
    2. 也稱作依賴關係, A指向B代表B依賴於A
    3. 通常也是因果關係, 但不一定是
    4. 圖模型的概率可以用
        1. 對先驗假設
        2. 使用相對頻率估計
        3. 使用MLE最大似然估計
2. 圖模型的聯合分部
    1. P(X2)代表X2在模型中不依賴於其他變數
    2. P(X3|X2,X6)代表X3在模型中依賴X2, X6
3. 贝叶斯网络常用技術
    1. 信念传播
    2. 蒙特卡罗采样
    3. 变分推理
4. HMM
    1. HMM適用於有序列的數據, 例如股市, 天氣
    2. 為優化計算, HMM會使用Viterbi Decoding維特比解碼
    3. 維特比解碼是一種動態解法, 在每一步計算選擇最有可能解繼續計算
    4. 數學表示HMM Λ={Θ,Ω,A,B,π}
        1. Θ隱狀態集合
        2. Ω輸出集合(觀察值)
        3. A狀態轉移概率矩陣: 一個狀態轉換成另一個狀態的機率, 有3個狀態就會有3*3矩陣
        4. B觀察概率矩陣: 一個狀態和觀察值之間的機率, 3個狀態4個觀察就會有3*4矩陣
        5. π初始狀態
5. HMM舉例
    1. 可觀察到的4個單辭, 編寫成100句段的段落, 一個單詞為一個句段
    2. 可能的標籤有3類
    3. 問題:
        1. 狀態轉移概率矩陣: 這裡狀態指的是標籤, 所以大小為3*3
        2. π初始狀態沒有特別註明: 均為1/3
        3. 觀察概率矩陣: 指的是可觀察和可能狀態, 所以大小為3*4
        4. 觀察大小: 有100個句段所以有100個觀察
        5. 狀態路徑: 同觀察大小, 每個可觀察句段會有一個隱狀態
        6. 已知第一個狀態: 每個觀察可能有3個狀態, 路徑剩下100-1, 剩下可能路徑為3^99
6. 圖模型計算: 列於紙上


# U6 Quiz
1. 無監督學習作用
    1. 樣本分組
    2. 辨識特徵
    3. 估計分布
2. 聚類
    1. 作用
        1. 找出結構
        2. 找到一致的數據組
        3. 可視化數據
    2. 實現良好聚類
        1. 找到對問題有意義的測量量
        2. 最小化簇內距離和(簇內都靠得很近)
3. GMM高斯混和模型
    1. 近似於許多實際數據分布
    2. 要學習前要給定成分(簇)的數量
    3. 屬於軟分配(不同簇的機率)
4. K均值算法
    1. 只用於聚類
    2. 屬性:
        1. 目標是最小化點到簇中心的平方距離
        2. 初始點對結果有很大的影響
        3. 屬於硬分配(非A即B)
        4. 簇中心不再更改則停止, 始終會在有限步數內收斂
        5. 可能只找到局部最優而非全局最優
        6. 和GMM一樣是無監督學習方法
    3. 優化方式
        1. 簇的數量
        2. 多次初始化(以減少初始短對結果的影響)
        3. 下一個點選擇距離最遠的點(k++算法)
        4. 注意異常值
    4. K均值計算: 列於紙上


# U7 Quiz
1. Spectral Clustering譜聚類
    1. 基於相似性的聚類
    2. 多會使用圖來表示
    3. 不會對簇的形式有強力的假設
    4. 對圖的參數選擇敏感
    5. 大數據的計算成本較高
    6. 連接矩陣W
        1. 對稱的二元矩陣
        2. 兩頂點相連W=1
        3. 也可以加上加權表示強度, 又稱為關聯矩陣
        4. 是簡單圖, 沒有和自己的連接, 對角為零
    7. 度矩陣D
        1. 是對角矩陣
        2. 代表頂點的邊數
    8. 拉普拉斯矩陣L
        1. 這裡指的是結合度矩陣和連接矩陣
        2. L= D - W
        3. 是對稱矩陣也是半正定矩陣
        4. 最小的非零特徵稱作譜隙
2. 圖割
    1. 最小割
        1. 在圖中以一條線切分不同點, 切分時以切除最少邊為優先
        2. 一般上會加上權重進行計算
        3. 只考慮割的規模, 所以可能造成切割不平均
        4. 二路劃分可慮第二小特徵向量
    2. 圖割的描述
        1. Cut(A,A): A簇內的所有邊不重複相加
        2. Vol(A): A簇內點的邊的算一次, 有四個點就把四個點的邊全部相加
        3. |A|: A簇內的節點數
    2. 比例割
        1. 最小割再乘上R(1分之|A|加1分之|B|): 假如節點平均, R會小於節點不平均
    3. 歸一化割
        1. 同比例割, 只是改用內部所有權重Vol(A)作為判斷
    4. 極小化極大割
        1. 同比例割, 只是改用內部連接Cut(A,A)作為判斷
3. 圖割的現實考慮
    1. 要分割成更多簇
        1. 遞歸/分層二分: 多次二分, 效率較低
        2. K位圖割: 一般化二元的目標函數, 也就是對應k^th的最小特徵值的向量
    2. 不同的拉普拉斯矩陣
    3. 不同的相似性圖構建
        1. ε領域圖
        2. k最近鄰圖
        3. 完全連接圖
    4. 簇的數量
        1. 特徵間隙啟發法: 選擇k讓特徵值較小, k+1較大
4. 優缺點
    1. 優點
        1. 不需要對簇有強力假設
        2. 只要是稀疏的就易於實現
        3. 良好的聚類結果
    2. 缺點
        1. 對參數選擇很敏感
        2. 大數據計算成本較高
        3. 最後階段會需要採用某種啟發法


# U8 Quiz
1. 需要降維的原因
    1. 維數災難: 更高維度往往需要更多樣本, 成指數成長
    2. 減少計算時間和數聚空間
    3. 低維度更易於可視化
2. 常見降維技術
    1. 特徵選取
    2. 特徵映射
        1. 獨立成分分析
        2. 主成分分析
        3. 非負矩陣分析
3. 降維的重構誤差要越小越好
4. 主成分分析
    1. 
   